{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoTl3GlI6FBR"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!pip install properscoring\n",
    "import properscoring as ps\n",
    "\n",
    "#!pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SbAJj2p7UnQ"
   },
   "outputs": [],
   "source": [
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(torch.__version__)\n",
    "print(ps.__version__)\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7qQ5sjF6FBZ"
   },
   "source": [
    "# Data loading & data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkFalqDS6FBZ"
   },
   "outputs": [],
   "source": [
    "# Load monthly MM data.\n",
    "# Transform the data into a lists of arrays. Each inner array represents a timeseries.\n",
    "# Remove all the NaN values from the datasets.\n",
    "\n",
    "# M4\n",
    "trainset = pd.read_csv('https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Train/Monthly-train.csv')\n",
    "testset = pd.read_csv('https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Test/Monthly-test.csv')\n",
    "trainset.set_index('V1', inplace = True)\n",
    "testset.set_index('V1', inplace = True)\n",
    "# Add the testset columns behind the trainset columns\n",
    "testset_merge = trainset.merge(testset, on = 'V1', how = 'inner')\n",
    "# Get the data in numpy representation\n",
    "trainset_np = trainset.values\n",
    "testset_np = testset_merge.values\n",
    "# Select all non NaN values from the trainset\n",
    "trainset_clean = [x[x == x] for x in trainset_np]\n",
    "# Train/validation/test --------------------------------- NBeats paper validation strategy\n",
    "testset_m4m = [x[x == x] for x in testset_np]\n",
    "valset_m4m = trainset_clean.copy()\n",
    "trainset_m4m = [x[:-18] for x in trainset_clean]\n",
    "\n",
    "del(trainset, testset, testset_merge, trainset_np, testset_np, trainset_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPcPq2FZ6FBa"
   },
   "source": [
    "# Optimization criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhyViHn46FBa"
   },
   "outputs": [],
   "source": [
    "# mean_forecasts = batch x fl\n",
    "# var_forecasts = batch x fl\n",
    "# targets = batch x fl\n",
    "# actuals_train = batch x bl\n",
    "\n",
    "# For training purposes\n",
    "def GaussianNLL(mean_forecasts, var_forecasts, targets):\n",
    "    NLL_items = torch.log(2*math.pi*var_forecasts)/2 + (1/(2*var_forecasts)) * (targets - mean_forecasts)**2\n",
    "    NLL_items_clamped = torch.clamp(NLL_items, -1e2, 1e2)#-1e3, 1e3)\n",
    "    NLL = torch.mean(NLL_items_clamped)\n",
    "    return NLL\n",
    "\n",
    "def GaussianKL(means_p, vars_p, means_q, vars_q):\n",
    "    var_ratios = vars_p / vars_q\n",
    "    t1s = ((means_p - means_q)**2 / vars_q)\n",
    "    KL_items = 0.5 * (var_ratios + t1s - 1 - var_ratios.log())\n",
    "    KL_items_clamped = torch.clamp(KL_items, 0, 1e2)#1e3)\n",
    "    KL = torch.mean(KL_items_clamped)\n",
    "    return KL\n",
    "\n",
    "def SMAPE_train(mean_forecasts, targets):\n",
    "    loss = 200 * torch.mean(torch.abs(targets - mean_forecasts) / (torch.abs(mean_forecasts).detach() + torch.abs(targets) + 1e-6))\n",
    "    return loss\n",
    "\n",
    "def RMSE(mean_forecasts, targets):\n",
    "    loss_items = torch.sqrt(torch.mean((targets - mean_forecasts)**2, dim = -1) + 1e-6)\n",
    "    loss = torch.mean(loss_items)\n",
    "    return loss\n",
    "\n",
    "def GaussianWD(means_p, vars_p, means_q, vars_q):\n",
    "    loss_items = torch.sqrt(torch.mean((means_p - means_q)**2 + vars_p + vars_q - 2*torch.sqrt(vars_p)*torch.sqrt(vars_q), dim = -1) + 1e-6)\n",
    "    loss = torch.mean(loss_items)\n",
    "    return loss\n",
    "\n",
    "# For evaluation purposes\n",
    "def SMAPE(mean_forecasts, targets):\n",
    "    loss = 200 * np.mean(np.abs(targets - mean_forecasts) / (np.abs(mean_forecasts) + np.abs(targets)))\n",
    "    return loss\n",
    "\n",
    "def MAPE(mean_forecasts, targets):\n",
    "    loss = 100 * np.mean(np.abs(targets - mean_forecasts) / targets)\n",
    "    return loss\n",
    "\n",
    "def RMSSE(mean_forecasts, targets, actuals_train):\n",
    "\n",
    "    mask = np.abs(actuals_train)>1e-6\n",
    "    scaling_denom = np.sum(mask, axis = -1) - 1\n",
    "    scaling_denom = np.where(scaling_denom > 0, scaling_denom, np.ones_like(scaling_denom))\n",
    "\n",
    "    scaling_data = (actuals_train[:, :, 1:] - actuals_train[:, :, :-1])**2\n",
    "    scaling_factor = np.sum(scaling_data, axis = -1) / scaling_denom\n",
    "    scaling_factor = np.where(scaling_factor > 0, scaling_factor, np.ones_like(scaling_factor))\n",
    "\n",
    "    scaling_factor_reshaped = np.expand_dims(scaling_factor, -1)\n",
    "    scaling_factor_reshaped = np.tile(scaling_factor_reshaped, targets.shape[-1])\n",
    "\n",
    "    loss_items = np.sqrt(np.mean((targets - mean_forecasts)**2 / scaling_factor_reshaped, axis = -1))\n",
    "    loss = np.mean(loss_items)\n",
    "    return loss\n",
    "\n",
    "def sCRPS(mean_forecasts, var_forecasts, targets, actuals_train):\n",
    "\n",
    "    mask = np.abs(actuals_train)>1e-6\n",
    "    scaling_denom = np.sum(mask, axis = -1) - 1\n",
    "    scaling_denom = np.where(scaling_denom > 0, scaling_denom, np.ones_like(scaling_denom))\n",
    "\n",
    "    scaling_data = np.abs(actuals_train[:, :, 1:] - actuals_train[:, :, :-1])\n",
    "    scaling_factor = np.sum(scaling_data, axis = -1) / scaling_denom\n",
    "    scaling_factor = np.where(scaling_factor > 0, scaling_factor, np.ones_like(scaling_factor))\n",
    "\n",
    "    scaling_factor_reshaped = np.expand_dims(scaling_factor, -1)\n",
    "    scaling_factor_reshaped = np.tile(scaling_factor_reshaped, targets.shape[-1])\n",
    "\n",
    "    sd_forecasts = np.sqrt(var_forecasts)\n",
    "    loss_items = np.mean(ps.crps_gaussian(targets, mean_forecasts, sd_forecasts) / scaling_factor_reshaped, axis = -1)\n",
    "    loss = np.mean(loss_items)\n",
    "    return loss\n",
    "\n",
    "def GaussianKL_eval(means_p, vars_p, means_q, vars_q):\n",
    "    var_ratios = vars_p / vars_q\n",
    "    t1s = ((means_p - means_q)**2 / vars_q)\n",
    "    KL_items = 0.5 * (var_ratios + t1s - 1 - np.log(var_ratios))\n",
    "    KL = np.mean(KL_items)\n",
    "    return KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeeAfyGu6FBb"
   },
   "source": [
    "# NBeats models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7fO_Q2S6FBb"
   },
   "source": [
    "Generic gaussian building block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBfTYv4G6FBb"
   },
   "outputs": [],
   "source": [
    "class GenericNBeatsGaussianBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 backcast_length,\n",
    "                 forecast_length,\n",
    "                 hidden_layer_units,\n",
    "                 thetas_dims,\n",
    "                 share_thetas,\n",
    "                 dropout = False,\n",
    "                 dropout_p = 0.0,\n",
    "                 neg_slope = 0.00):\n",
    "\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        if isinstance(hidden_layer_units, int):\n",
    "            self.hidden_layer_units = [hidden_layer_units for FC_layer in range(4)]\n",
    "        else:\n",
    "            #assert(len(hidden_layer_units) == 4)\n",
    "            self.hidden_layer_units = hidden_layer_units\n",
    "        self.thetas_dims = thetas_dims\n",
    "        self.share_thetas = share_thetas\n",
    "        self.dropout = dropout\n",
    "        self.dropout_p = dropout_p\n",
    "        self.neg_slope = neg_slope\n",
    "\n",
    "        # shared layers in block\n",
    "        self.fc1 = nn.Linear(self.backcast_length,\n",
    "                             self.hidden_layer_units[0])#, bias = False)\n",
    "        self.fc2 = nn.Linear(self.hidden_layer_units[0], self.hidden_layer_units[1])#, bias = False)\n",
    "        self.fc3 = nn.Linear(self.hidden_layer_units[1], self.hidden_layer_units[2])#, bias = False)\n",
    "        self.fc4 = nn.Linear(self.hidden_layer_units[2], self.hidden_layer_units[3])#, bias = False)\n",
    "\n",
    "        # do not use F.dropout as you want dropout to only affect training (not evaluation mode)\n",
    "        # nn.Dropout handles this automatically\n",
    "        if self.dropout:\n",
    "            self.dropoutlayer = nn.Dropout(p = self.dropout_p)\n",
    "\n",
    "        # task specific (backcast & forecast) layers in block\n",
    "        # do not include bias - see section 3.1 - Ruben does include bias for generic blocks\n",
    "        if self.share_thetas:\n",
    "            self.theta_bc = self.theta_mean_fc = self.theta_var_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
    "        else:\n",
    "            self.theta_bc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
    "            self.theta_mean_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
    "            self.theta_var_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
    "\n",
    "        # block output layers\n",
    "        self.backcast_out = nn.Linear(self.thetas_dims, self.backcast_length)#, bias = False) # include bias - see section 3.3\n",
    "        self.mean_forecast_out = nn.Linear(self.thetas_dims, self.forecast_length)#, bias = False) # include bias - see section 3.3\n",
    "        self.var_forecast_out = nn.Linear(self.thetas_dims, self.forecast_length)#, bias = False) # include bias - see section 3.3\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.dropout:\n",
    "\n",
    "            h1 = F.leaky_relu(self.fc1(x.to(self.device)), negative_slope = self.neg_slope)\n",
    "            h1 = self.dropoutlayer(h1)\n",
    "            h2 = F.leaky_relu(self.fc2(h1), negative_slope = self.neg_slope)\n",
    "            h2 = self.dropoutlayer(h2)\n",
    "            h3 = F.leaky_relu(self.fc3(h2), negative_slope = self.neg_slope)\n",
    "            h3 = self.dropoutlayer(h3)\n",
    "            h4 = F.leaky_relu(self.fc4(h3), negative_slope = self.neg_slope)\n",
    "\n",
    "            theta_b = F.leaky_relu(self.theta_bc(h4), negative_slope = self.neg_slope)\n",
    "            theta_mean_f = F.leaky_relu(self.theta_mean_fc(h4), negative_slope = self.neg_slope)\n",
    "            theta_var_f = F.leaky_relu(self.theta_var_fc(h4), negative_slope = self.neg_slope)\n",
    "\n",
    "            backcast = self.backcast_out(theta_b)\n",
    "            mean_forecast = self.mean_forecast_out(theta_mean_f)\n",
    "            #var_forecast = F.softplus(self.var_forecast_out(theta_var_f), beta = 1)\n",
    "            var_forecast = F.softplus(self.var_forecast_out(theta_var_f), beta = 1)**2\n",
    "            #var_forecast = F.relu(self.var_forecast_out(theta_var_f)) + 1e-6\n",
    "\n",
    "        else:\n",
    "\n",
    "            h1 = F.leaky_relu(self.fc1(x.to(self.device)), negative_slope = self.neg_slope)\n",
    "            h2 = F.leaky_relu(self.fc2(h1), negative_slope = self.neg_slope)\n",
    "            h3 = F.leaky_relu(self.fc3(h2), negative_slope = self.neg_slope)\n",
    "            h4 = F.leaky_relu(self.fc4(h3), negative_slope = self.neg_slope)\n",
    "\n",
    "            theta_b = F.leaky_relu(self.theta_bc(h4), negative_slope = self.neg_slope)\n",
    "            theta_mean_f = F.leaky_relu(self.theta_mean_fc(h4), negative_slope = self.neg_slope)\n",
    "            theta_var_f = F.leaky_relu(self.theta_var_fc(h4), negative_slope = self.neg_slope)\n",
    "\n",
    "            backcast = self.backcast_out(theta_b)\n",
    "            mean_forecast = self.mean_forecast_out(theta_mean_f)\n",
    "            #var_forecast = F.softplus(self.var_forecast_out(theta_var_f), beta = 1)\n",
    "            var_forecast = F.softplus(self.var_forecast_out(theta_var_f), beta = 1)**2\n",
    "            #var_forecast = F.relu(self.var_forecast_out(theta_var_f)) + 1e-6\n",
    "\n",
    "        return backcast, mean_forecast, var_forecast\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "\n",
    "        block_type = type(self).__name__\n",
    "\n",
    "        return f'{block_type}(units={self.hidden_layer_units}, thetas_dims={self.thetas_dims}, ' \\\n",
    "            f'backcast_length={self.backcast_length}, ' \\\n",
    "            f'forecast_length={self.forecast_length}, share_thetas={self.share_thetas}, ' \\\n",
    "            f'dropout={self.dropout}, dropout_p={self.dropout_p}, neg_slope={self.neg_slope}) at @{id(self)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9cpqTa96FBg"
   },
   "source": [
    "StableNBeatsGaussianNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-r2Uol_6FBk"
   },
   "outputs": [],
   "source": [
    "# Only the forward method is changed compared to standard NBeatsNet\n",
    "class StableNBeatsGaussianNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 backcast_length_multiplier,\n",
    "                 forecast_length,\n",
    "                 hidden_layer_units,\n",
    "                 thetas_dims,\n",
    "                 share_thetas,\n",
    "                 nb_blocks_per_stack,\n",
    "                 n_stacks,\n",
    "                 share_weights_in_stack,\n",
    "                 dropout = False,\n",
    "                 dropout_p = 0.0,\n",
    "                 neg_slope = 0.00):\n",
    "\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.backcast_length = backcast_length_multiplier * forecast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.hidden_layer_units = hidden_layer_units\n",
    "        self.thetas_dims = thetas_dims\n",
    "        self.share_thetas = share_thetas\n",
    "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
    "        self.n_stacks = n_stacks\n",
    "        self.share_weights_in_stack = share_weights_in_stack\n",
    "        self.dropout = dropout\n",
    "        self.dropout_p = dropout_p\n",
    "        self.neg_slope = neg_slope\n",
    "\n",
    "        self.stacks = []\n",
    "        self.parameters = []\n",
    "\n",
    "        print(f'| N-Beats')\n",
    "        for stack_id in range(self.n_stacks):\n",
    "            self.stacks.append(self.create_stack(stack_id))\n",
    "        self.parameters = nn.ParameterList(self.parameters)\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def create_stack(self, stack_id):\n",
    "\n",
    "        print(f'| --  Stack Generic (#{stack_id}) (share_weights_in_stack={self.share_weights_in_stack})')\n",
    "        blocks = []\n",
    "        for block_id in range(self.nb_blocks_per_stack):\n",
    "            if self.share_weights_in_stack and block_id != 0:\n",
    "                block = blocks[-1]  # pick up the last one when we share weights\n",
    "            else:\n",
    "                block = GenericNBeatsGaussianBlock(self.device,\n",
    "                                                   self.backcast_length,\n",
    "                                                   self.forecast_length,\n",
    "                                                   self.hidden_layer_units,\n",
    "                                                   self.thetas_dims,\n",
    "                                                   self.share_thetas,\n",
    "                                                   self.dropout,\n",
    "                                                   self.dropout_p,\n",
    "                                                   self.neg_slope)\n",
    "                self.parameters.extend(block.parameters())\n",
    "                print(f'     | -- {block}')\n",
    "                blocks.append(block)\n",
    "\n",
    "        return blocks\n",
    "\n",
    "\n",
    "    def forward(self, backcast_arr):\n",
    "\n",
    "        # dim backcast_arr = batch_size x shifts x backcast_length\n",
    "        # shifts == 0 is standard input window, others are shifted lookback windows\n",
    "        # higher index = further back in time\n",
    "        # feed different input windows (per batch) through the SAME network (check via list of learnable parameters)\n",
    "        # see https://stackoverflow.com/questions/54444630/application-of-nn-linear-layer-in-pytorch-on-additional-dimentions\n",
    "\n",
    "        mean_forecast_arr = torch.zeros((backcast_arr.shape[0], # take batch size from backcast\n",
    "                                         backcast_arr.shape[1], # take n of shifts from backcast\n",
    "                                         self.forecast_length), dtype = torch.float).to(self.device)\n",
    "\n",
    "        var_forecast_arr = torch.zeros((backcast_arr.shape[0], # take batch size from backcast\n",
    "                                        backcast_arr.shape[1], # take n of shifts from backcast\n",
    "                                        self.forecast_length), dtype = torch.float).to(self.device)\n",
    "\n",
    "        # loop through stacks (and blocks)\n",
    "        for stack_id in range(len(self.stacks)):\n",
    "            for block_id in range(len(self.stacks[stack_id])):\n",
    "                b, m_f, var_f = self.stacks[stack_id][block_id](backcast_arr)\n",
    "                backcast_arr = backcast_arr.to(self.device) - b\n",
    "                mean_forecast_arr = mean_forecast_arr.to(self.device) + m_f\n",
    "                var_forecast_arr = var_forecast_arr.to(self.device) + var_f\n",
    "\n",
    "        return backcast_arr, mean_forecast_arr, var_forecast_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIfSD8_k6FBl"
   },
   "source": [
    "\n",
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-EzkGDL6FBm"
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed = 5101992):\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjJUVax_6FBm"
   },
   "outputs": [],
   "source": [
    "class StableNBeatsGaussianLearner:\n",
    "\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 forecast_length,\n",
    "                 configNBeats):\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        self.device = device\n",
    "        self.forecast_length = forecast_length\n",
    "        self.configNBeats = configNBeats\n",
    "\n",
    "        self.rndseed = self.configNBeats[\"rndseed\"]\n",
    "\n",
    "        seed_torch(self.rndseed)\n",
    "\n",
    "        print('--- Model ---')\n",
    "        self.model = StableNBeatsGaussianNet(self.device,\n",
    "                                             self.configNBeats[\"backcast_length_multiplier\"],\n",
    "                                             self.forecast_length,\n",
    "                                             self.configNBeats[\"hidden_layer_units\"],\n",
    "                                             self.configNBeats[\"thetas_dims\"],\n",
    "                                             self.configNBeats[\"share_thetas\"],\n",
    "                                             self.configNBeats[\"nb_blocks_per_stack\"],\n",
    "                                             self.configNBeats[\"n_stacks\"],\n",
    "                                             self.configNBeats[\"share_weights_in_stack\"],\n",
    "                                             self.configNBeats[\"dropout\"],\n",
    "                                             self.configNBeats[\"dropout_p\"],\n",
    "                                             self.configNBeats[\"neg_slope\"])\n",
    "\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(),\n",
    "                                      lr = self.configNBeats[\"learning_rate\"],\n",
    "                                      weight_decay = self.configNBeats[\"weight_decay\"])\n",
    "\n",
    "        self.init_state = copy.deepcopy(self.model.state_dict())\n",
    "        self.init_state_opt = copy.deepcopy(self.optim.state_dict())\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        wandb.watch(self.model)\n",
    "\n",
    "\n",
    "    def ts_padding(self, ts_train_data, ts_eval_data):\n",
    "\n",
    "        # Some time series in the dataset are not long enough to support the specified:\n",
    "        # forecast_length and backcast_length + backcast input shifts\n",
    "        # we use zero padding for the time series that are too short (neutral effect on loss calculations)\n",
    "        # + self.shifts comes from the number of extra observations needed to create the shifted inputs/targets\n",
    "        # + (self.forgins - 1) comes from rolling origin evaluation\n",
    "\n",
    "        length_train = (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length +\n",
    "                        self.forecast_length +\n",
    "                        self.shifts)\n",
    "        length_eval = (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length +\n",
    "                       self.forecast_length +\n",
    "                       self.shifts +\n",
    "                       (self.forigins - 1))\n",
    "\n",
    "        ts_train_pad = [x if x.size >= length_train else np.pad(x,\n",
    "                                                                (int(length_train - x.size), 0),\n",
    "                                                                'constant',\n",
    "                                                                constant_values = 0) for x in ts_train_data]\n",
    "        ts_eval_pad = [x if x.size >= length_eval else np.pad(x,\n",
    "                                                              (int(length_eval - x.size), 0),\n",
    "                                                              'constant',\n",
    "                                                              constant_values = 0) for x in ts_eval_data]\n",
    "\n",
    "        return ts_train_pad, ts_eval_pad\n",
    "\n",
    "\n",
    "    def make_batch(self, batch_data, shuffle_origin = True):\n",
    "\n",
    "        # If shuffle_origin = True --> batch for training --> random forecast origin based on LH\n",
    "        # If shuffle_origin = False --> batch for evaluation --> fixed forecast origin\n",
    "\n",
    "        # Split the batch into input_list and target_list\n",
    "        # In x_arr and target_arr: batch x shift x backcats_length/forecast_length\n",
    "        x_arr = np.empty(shape = (len(batch_data),\n",
    "                                  self.shifts + 1,\n",
    "                                  self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length))\n",
    "        target_arr = np.empty(shape = (len(batch_data),\n",
    "                                       self.shifts + 1,\n",
    "                                       self.forecast_length))\n",
    "\n",
    "        # For every time series in the batch:\n",
    "        # (1) slice the time series according to specific forecasting origin (depending on shuffle_origin)\n",
    "        # (2) make shifted inputs/targets --> max number of shifts = forecast_length - 1\n",
    "        # (3) fill x_arr and target_arr\n",
    "        for j in range(len(batch_data)):\n",
    "            i = batch_data[j]\n",
    "\n",
    "            if shuffle_origin:\n",
    "                # suffle_origin --> only in training\n",
    "\n",
    "\n",
    "                # pick origin\n",
    "                LH_max_offset = int(self.configNBeats[\"LH\"] * self.forecast_length)\n",
    "                ts_max_offset = int(len(i) -\n",
    "                                    (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length +\n",
    "                                     self.forecast_length +\n",
    "                                     self.shifts))\n",
    "                max_offset = min(LH_max_offset, ts_max_offset)\n",
    "                if max_offset < 1:\n",
    "                    offset = np.zeros(1)\n",
    "                else:\n",
    "                    offset = np.random.randint(low = 0, high = max_offset)\n",
    "            else:\n",
    "                offset = np.zeros(1)\n",
    "\n",
    "            if offset == 0:\n",
    "                for shift in range(self.shifts + 1):\n",
    "                    if shift == 0:\n",
    "                        x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length:-self.forecast_length]\n",
    "                        target_arr[j, shift, :] = i[-self.forecast_length:]\n",
    "                    else:\n",
    "                        x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length-shift:-self.forecast_length-shift]\n",
    "                        target_arr[j, shift, :] = i[-self.forecast_length-shift:-shift]\n",
    "            else:\n",
    "                for shift in range(self.shifts + 1):\n",
    "                    x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length-offset-shift:-self.forecast_length-offset-shift]\n",
    "                    target_arr[j, shift, :] = i[-self.forecast_length-offset-shift:-offset-shift]\n",
    "\n",
    "        scalers_arr = np.mean(x_arr, axis = (1,2))+1\n",
    "        # Use same scaling factor for all shifted versions of time series\n",
    "        # I think this is important for stability calculations!\n",
    "\n",
    "        scalers_arr_reshaped = np.expand_dims(scalers_arr, (1,2))\n",
    "\n",
    "        x_arr = x_arr / scalers_arr_reshaped\n",
    "        target_arr = target_arr / scalers_arr_reshaped\n",
    "\n",
    "        #print(target_arr)\n",
    "\n",
    "        return x_arr, target_arr, scalers_arr_reshaped\n",
    "\n",
    "\n",
    "    def create_example_plots(self, mean_forecast, var_forecast, target, actuals_train, final_evaluation = False):\n",
    "\n",
    "        plot_forecasts = torch.cat((actuals_train, mean_forecast))\n",
    "        plot_LQ1SD = torch.cat((actuals_train, mean_forecast - 1*torch.sqrt(var_forecast)))\n",
    "        plot_LQ2SD = torch.cat((actuals_train, mean_forecast - 2*torch.sqrt(var_forecast)))\n",
    "        plot_UQ1SD = torch.cat((actuals_train, mean_forecast + 1*torch.sqrt(var_forecast)))\n",
    "        plot_UQ2SD = torch.cat((actuals_train, mean_forecast + 2*torch.sqrt(var_forecast)))\n",
    "        plot_actuals = torch.cat((actuals_train, target))\n",
    "\n",
    "        random_sample_forecasts = plot_forecasts.squeeze().numpy().tolist()\n",
    "        random_sample_LQ1SD = plot_LQ1SD.squeeze().numpy().tolist()\n",
    "        random_sample_LQ2SD = plot_LQ2SD.squeeze().numpy().tolist()\n",
    "        random_sample_UQ1SD = plot_UQ1SD.squeeze().numpy().tolist()\n",
    "        random_sample_UQ2SD = plot_UQ2SD.squeeze().numpy().tolist()\n",
    "        random_sample_actuals = plot_actuals.squeeze().numpy().tolist()\n",
    "\n",
    "        x_axis = torch.arange(1, len(random_sample_forecasts)+1).numpy().tolist()\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        #'''\n",
    "        fig.add_traces(go.Scatter(x = x_axis,\n",
    "                                  y = random_sample_UQ2SD,\n",
    "                                  fill = None,\n",
    "                                  mode = 'lines',\n",
    "                                  name = '+2SD',\n",
    "                                  line_color = 'rgba(255,0,0,0.2)',\n",
    "                                  showlegend = False))\n",
    "\n",
    "        fig.add_traces(go.Scatter(x = x_axis,\n",
    "                                  y = random_sample_LQ2SD,\n",
    "                                  fill = 'tonexty',\n",
    "                                  mode = 'lines',\n",
    "                                  name = '-2SD',\n",
    "                                  line_color = 'rgba(255,0,0,0.2)',\n",
    "                                  fillcolor = 'rgba(255,0,0,0.2)',\n",
    "                                  showlegend = False))\n",
    "        #'''\n",
    "\n",
    "        fig.add_traces(go.Scatter(x = x_axis,\n",
    "                                  y = random_sample_UQ1SD,\n",
    "                                  fill = None,\n",
    "                                  mode = 'lines',\n",
    "                                  name = '+1SD',\n",
    "                                  line_color = 'rgba(255,0,0,0.5)',\n",
    "                                  showlegend = False))\n",
    "\n",
    "        fig.add_traces(go.Scatter(x = x_axis,\n",
    "                                  y = random_sample_LQ1SD,\n",
    "                                  fill = 'tonexty',\n",
    "                                  mode = 'lines',\n",
    "                                  name = '-1SD',\n",
    "                                  line_color = 'rgba(255,0,0,0.5)',\n",
    "                                  fillcolor = 'rgba(255,0,0,0.5)',\n",
    "                                  showlegend = False))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x = x_axis,\n",
    "                                 y = random_sample_forecasts,\n",
    "                                 mode = 'lines',\n",
    "                                 name = 'forecasts',\n",
    "                                 line_color = 'rgb(255,0,0)'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x = x_axis,\n",
    "                                 y = random_sample_actuals,\n",
    "                                 mode = 'lines',\n",
    "                                 name = 'actuals',\n",
    "                                 line_color = 'rgb(0,0,0)'))\n",
    "\n",
    "        # We only visualize examples for last epoch\n",
    "        if not final_evaluation:\n",
    "            wandb.log({\"example_plots_evaluation\": fig})\n",
    "        else:\n",
    "            wandb.log({\"example_plots_final_evaluation\": fig})\n",
    "\n",
    "\n",
    "    def evaluate(self, x_arr, target_arr,\n",
    "                 epoch = None,\n",
    "                 need_grad = True,\n",
    "                 early_stop = False):\n",
    "\n",
    "        losses = dict()\n",
    "\n",
    "        # Inputs must be converted to np.array of Tensors (float)\n",
    "        x_arr = torch.from_numpy(x_arr).float().to(self.device)\n",
    "        target_arr = torch.from_numpy(target_arr).float().to(self.device)\n",
    "\n",
    "        if need_grad:\n",
    "            self.model.train()\n",
    "            self.model.to(self.device)\n",
    "            _, mean_forecast_arr, var_forecast_arr = self.model(x_arr)\n",
    "\n",
    "            NLL_shifts = 0.0\n",
    "            for shift in range(self.shifts + 1):\n",
    "                NLL_shifts += GaussianNLL(mean_forecast_arr[:, shift, :],\n",
    "                                          var_forecast_arr[:, shift, :],\n",
    "                                          target_arr[:, shift, :])\n",
    "            losses[\"accuracy\"] = NLL_shifts / (self.shifts + 1)\n",
    "\n",
    "            if self.shifts > 0:\n",
    "                # dimensions = batch_size x shifted forecast distributions for stability computations\n",
    "                mean_forecast_base_arr = torch.zeros((mean_forecast_arr.shape[0],\n",
    "                                                      sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                     dtype = torch.float)\n",
    "                var_forecast_base_arr = torch.zeros((var_forecast_arr.shape[0],\n",
    "                                                     sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                    dtype = torch.float)\n",
    "                mean_forecast_shift_arr = torch.zeros((mean_forecast_arr.shape[0],\n",
    "                                                       sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                      dtype = torch.float)\n",
    "                var_forecast_shift_arr = torch.zeros((var_forecast_arr.shape[0],\n",
    "                                                      sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                     dtype = torch.float)\n",
    "                col = 0\n",
    "                for shift in range(1, self.shifts + 1):\n",
    "                    for horizon_m1 in range(self.forecast_length - shift):\n",
    "                        mean_forecast_base_arr[:, col] = mean_forecast_arr[:, 0, horizon_m1]\n",
    "                        var_forecast_base_arr[:, col] = var_forecast_arr[:, 0, horizon_m1]\n",
    "                        mean_forecast_shift_arr[:, col] = mean_forecast_arr[:, shift, horizon_m1 + shift]\n",
    "                        var_forecast_shift_arr[:, col] = var_forecast_arr[:, shift, horizon_m1 + shift]\n",
    "                        col = col + 1\n",
    "                \n",
    "                \n",
    "                if self.configNBeats[\"training_stability\"] == \"WD\":\n",
    "                    \n",
    "                    #print(\"WD\")\n",
    "                    losses[\"stability\"] = GaussianWD(mean_forecast_base_arr, var_forecast_base_arr,\n",
    "                                                         mean_forecast_shift_arr, var_forecast_shift_arr)\n",
    "                elif self.configNBeats[\"training_stability\"] == \"KL\":\n",
    "                    #print(\"KL\")\n",
    "                    losses[\"stability\"] = (0.5 * GaussianKL(mean_forecast_base_arr, var_forecast_base_arr,\n",
    "                                                            mean_forecast_shift_arr, var_forecast_shift_arr) +\n",
    "                                           0.5 * GaussianKL(mean_forecast_shift_arr, var_forecast_shift_arr,\n",
    "                                                            mean_forecast_base_arr, var_forecast_base_arr))\n",
    "                elif self.configNBeats[\"training_stability\"] == \"RMSC\":\n",
    "                    #print(\"RMSC\")\n",
    "                    losses[\"stability\"] = RMSE(mean_forecast_base_arr, mean_forecast_shift_arr)\n",
    "            else:\n",
    "                losses[\"stability\"] = torch.zeros(1)\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                self.model.to(self.device)\n",
    "                _, mean_forecast_arr, var_forecast_arr = self.model(x_arr)\n",
    "\n",
    "                NLL_shifts = 0.0\n",
    "                for shift in range(self.shifts + 1):\n",
    "                    NLL_shifts += GaussianNLL(mean_forecast_arr[:, shift, :],\n",
    "                                              var_forecast_arr[:, shift, :],\n",
    "                                              target_arr[:, shift, :])\n",
    "                losses[\"accuracy\"] = NLL_shifts / (self.shifts + 1)\n",
    "\n",
    "                if self.shifts > 0:\n",
    "                    mean_forecast_base_arr = torch.zeros((mean_forecast_arr.shape[0],\n",
    "                                                      sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                     dtype = torch.float)\n",
    "                    var_forecast_base_arr = torch.zeros((var_forecast_arr.shape[0],\n",
    "                                                         sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                        dtype = torch.float)\n",
    "                    mean_forecast_shift_arr = torch.zeros((mean_forecast_arr.shape[0],\n",
    "                                                           sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                          dtype = torch.float)\n",
    "                    var_forecast_shift_arr = torch.zeros((var_forecast_arr.shape[0],\n",
    "                                                          sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                         dtype = torch.float)\n",
    "                    col = 0\n",
    "                    for shift in range(1, self.shifts + 1):\n",
    "                        for horizon_m1 in range(self.forecast_length - shift):\n",
    "                            mean_forecast_base_arr[:, col] = mean_forecast_arr[:, 0, horizon_m1]\n",
    "                            var_forecast_base_arr[:, col] = var_forecast_arr[:, 0, horizon_m1]\n",
    "                            mean_forecast_shift_arr[:, col] = mean_forecast_arr[:, shift, horizon_m1 + shift]\n",
    "                            var_forecast_shift_arr[:, col] = var_forecast_arr[:, shift, horizon_m1 + shift]\n",
    "                            col = col + 1\n",
    "                    \n",
    "                    if self.configNBeats[\"training_stability\"] == \"WD\":\n",
    "                        \n",
    "                        losses[\"stability\"] = GaussianWD(mean_forecast_base_arr, var_forecast_base_arr,\n",
    "                                                         mean_forecast_shift_arr, var_forecast_shift_arr)\n",
    "                    elif self.configNBeats[\"training_stability\"] == \"KL\":\n",
    "                        losses[\"stability\"] = (0.5 * GaussianKL(mean_forecast_base_arr, var_forecast_base_arr,\n",
    "                                                            mean_forecast_shift_arr, var_forecast_shift_arr) +\n",
    "                                           0.5 * GaussianKL(mean_forecast_shift_arr, var_forecast_shift_arr,\n",
    "                                                            mean_forecast_base_arr, var_forecast_base_arr))\n",
    "                    elif self.configNBeats[\"training_stability\"] == \"RMSC\":\n",
    "                        losses[\"stability\"] = RMSE(mean_forecast_base_arr, mean_forecast_shift_arr)\n",
    "                else:\n",
    "                    losses[\"stability\"] = torch.zeros(1)\n",
    "\n",
    "                if not self.disable_plot:\n",
    "                    # Plot validation examples - of standard/unshifted input - for last epoch\n",
    "                    # This part of the evaluation function is called after training has been completed\n",
    "                    if (epoch == self.configNBeats[\"epochs\"]):\n",
    "                        self.create_example_plots(mean_forecast_arr[0, 0, :], var_forecast_arr[0, 0, :],\n",
    "                                                  target_arr[0, 0, :], x_arr[0, 0, :])\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "    # Training of net (training data can include validation data) + validation or testing\n",
    "    def train_net(self,\n",
    "                  ts_train_mm,\n",
    "                  ts_eval_mm,\n",
    "                  forigins,\n",
    "                  validation = True,\n",
    "                  disable_plot = True):\n",
    "\n",
    "        self.forigins = forigins\n",
    "        self.shifts = self.configNBeats[\"shifts\"]\n",
    "        self.validation = validation\n",
    "        self.disable_plot = disable_plot\n",
    "        #assert self.shifts < self.forecast_length # max allowed number of shifts is forecast_length - 1\n",
    "\n",
    "        # Data preprocessing depends on backcast_length_multiplier\n",
    "        ts_train_pad, ts_eval_pad = self.ts_padding(ts_train_mm, ts_eval_mm)\n",
    "        ts_train_pad = np.array(ts_train_pad, dtype = object)\n",
    "        ts_eval_pad = np.array(ts_eval_pad, dtype = object)\n",
    "\n",
    "        print('--- Training ---')\n",
    "\n",
    "        # Containers to save train/evaluation losses and parameters\n",
    "        tloss_combined, tloss_accuracy, tloss_stability = [], [], []\n",
    "        eloss_combined, eloss_accuracy, eloss_stability = [], [], []\n",
    "        #params = []\n",
    "\n",
    "        # Main training loop\n",
    "        self.model.load_state_dict(self.init_state)\n",
    "        self.optim.load_state_dict(self.init_state_opt)\n",
    "\n",
    "        seed_torch(self.rndseed)\n",
    "\n",
    "        for epoch in range(1, self.configNBeats[\"epochs\"]+1):\n",
    "\n",
    "            start_time = time()\n",
    "            # Shuffle train data\n",
    "            np.random.shuffle(ts_train_pad)\n",
    "            # Determine number of batches per epoch\n",
    "            num_batches = int(ts_train_pad.shape[0] / self.configNBeats[\"batch_size\"])\n",
    "\n",
    "            # Training per epoch\n",
    "            avg_tloss_combined_epoch = 0.0\n",
    "            avg_tloss_accuracy_epoch = 0.0\n",
    "            avg_tloss_stability_epoch = 0.0\n",
    "\n",
    "            for k in range(num_batches):\n",
    "\n",
    "                batch = np.array(ts_train_pad[k*self.configNBeats[\"batch_size\"]:(k+1)*self.configNBeats[\"batch_size\"]])\n",
    "                x_arr, target_arr, _ = self.make_batch(batch, shuffle_origin = True)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                losses_batch = self.evaluate(x_arr, target_arr,\n",
    "                                             epoch, need_grad = True,\n",
    "                                             early_stop = False)\n",
    "                if self.shifts > 0:\n",
    "                    loss_combined = ((self.configNBeats[\"lambda\"] * losses_batch[\"stability\"]) +\n",
    "                                     ((1 - self.configNBeats[\"lambda\"]) * losses_batch[\"accuracy\"]))\n",
    "                else:\n",
    "                    loss_combined = losses_batch[\"accuracy\"]\n",
    "                loss_combined.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "\n",
    "                #params = self.model.parameters()\n",
    "                #total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "                #if (epoch == 1 or epoch == self.configNBeats[\"epochs\"]) and k == 0:\n",
    "                #    print('Epoch {}/{} \\t n_learnable_pars={:.4f}'.format(\n",
    "                #        epoch,\n",
    "                #        self.configNBeats[\"epochs\"],\n",
    "                #        total_params))\n",
    "\n",
    "                wandb.log({\"tloss_comb_step\": loss_combined,\n",
    "                           \"tloss_acc_step\": losses_batch[\"accuracy\"],\n",
    "                           \"tloss_stab_step\": losses_batch[\"stability\"]})\n",
    "\n",
    "                avg_tloss_combined_epoch += (loss_combined / num_batches)\n",
    "                avg_tloss_accuracy_epoch += (losses_batch[\"accuracy\"] / num_batches)\n",
    "                avg_tloss_stability_epoch += (losses_batch[\"stability\"] / num_batches)\n",
    "\n",
    "\n",
    "            elapsed_time = time() - start_time\n",
    "\n",
    "            print('Epoch {}/{} \\t tloss_combined={:.4f} \\t time={:.2f}s'.format(\n",
    "                epoch,\n",
    "                self.configNBeats[\"epochs\"],\n",
    "                avg_tloss_combined_epoch,\n",
    "                elapsed_time))\n",
    "\n",
    "            wandb.log({\"epoch\": epoch,\n",
    "                        \"tloss_comb_evol\": avg_tloss_combined_epoch,\n",
    "                        \"tloss_acc_evol\": avg_tloss_accuracy_epoch,\n",
    "                        \"tloss_stab_evol\": avg_tloss_stability_epoch})\n",
    "\n",
    "        wandb.log({\"tloss_comb\": avg_tloss_combined_epoch,\n",
    "                   \"tloss_acc\": avg_tloss_accuracy_epoch,\n",
    "                   \"tloss_stab\": avg_tloss_stability_epoch})\n",
    "\n",
    "        print('--- Training done ---')\n",
    "        print('--- Final evaluation ---')\n",
    "\n",
    "        # Containers to save actuals and forecasts\n",
    "        #'''\n",
    "        actuals = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        mean_forecasts = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        mean_forecasts_shifted = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        var_forecasts = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        var_forecasts_shifted = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        actuals_train = np.empty(shape = (len(ts_eval_pad), self.forigins,\n",
    "                                          self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length)) # n_series, forigin, backcast_length\n",
    "        #'''\n",
    "\n",
    "        actuals_scaled = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        actuals_shifted_scaled = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        mean_forecasts_scaled = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        mean_forecasts_shifted_scaled = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        var_forecasts_scaled = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        var_forecasts_shifted_scaled = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        actuals_train_scaled = np.empty(shape = (len(ts_eval_pad), self.forigins,\n",
    "                                                 self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length)) # n_series, forigin, backcast_length\n",
    "\n",
    "        # Forecasts for each origin in rolling_window\n",
    "        for forigin in range(self.forigins):\n",
    "\n",
    "            # Only one batch, but one batch per forecast origin\n",
    "            if forigin < self.forigins-1:\n",
    "                eval_data_subset = np.array([x[:(-18 + forigin + self.forecast_length)] for x in ts_eval_pad],\n",
    "                                           dtype = object)\n",
    "            else:\n",
    "                eval_data_subset = np.array([x for x in ts_eval_pad], dtype = object)\n",
    "\n",
    "            x_arr, target_arr, scalers_arr_reshaped = self.make_batch(eval_data_subset, shuffle_origin = False)\n",
    "\n",
    "            # Produce forecasts for subset of test data\n",
    "            x_arr = torch.from_numpy(x_arr).float().to(self.device)\n",
    "            target_arr = torch.from_numpy(target_arr).float().to(self.device)\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                self.model.to(self.device)\n",
    "                _, mean_forecast_arr, var_forecast_arr = self.model(x_arr)\n",
    "\n",
    "            #RESCALING#######################################################################################\n",
    "            scalers = torch.from_numpy(scalers_arr_reshaped).float().to(self.device)\n",
    "\n",
    "            mean_forecast_arr_scaled = mean_forecast_arr * scalers\n",
    "            var_forecast_arr_scaled = var_forecast_arr * scalers**2\n",
    "\n",
    "            target_arr_scaled = target_arr * scalers\n",
    "            x_arr_scaled = x_arr * scalers\n",
    "            #RESCALING#######################################################################################\n",
    "\n",
    "            # Plot 10 random examples per origin - of standard/unshifted input\n",
    "            #sample_ids = np.random.randint(low = 0, high = int(x_arr_scaled.shape[0]), size = 10)\n",
    "            sample_ids = np.arange(2)#10)\n",
    "            for sample_id in sample_ids:\n",
    "                '''\n",
    "                self.create_example_plots(mean_forecast_arr[sample_id, 0, :],\n",
    "                                          var_forecast_arr[sample_id, 0, :],\n",
    "                                          target_arr[sample_id, 0, :],\n",
    "                                          x_arr[sample_id, 0, :],\n",
    "                                          final_evaluation = True)\n",
    "                '''\n",
    "                self.create_example_plots(mean_forecast_arr_scaled[sample_id, 0, :].cpu(),\n",
    "                                          var_forecast_arr_scaled[sample_id, 0, :].cpu(),\n",
    "                                          target_arr_scaled[sample_id, 0, :].cpu(),\n",
    "                                          x_arr_scaled[sample_id, 0, :].cpu(),\n",
    "                                          final_evaluation = True)\n",
    "\n",
    "            # Save to containers\n",
    "            #'''\n",
    "            mean_forecasts[:, forigin, :] = mean_forecast_arr[:, 0, :].cpu()\n",
    "            mean_forecasts_shifted[:, forigin, :] = mean_forecast_arr[:, 1, :].cpu()\n",
    "            var_forecasts[:, forigin, :] = var_forecast_arr[:, 0, :].cpu()\n",
    "            var_forecasts_shifted[:, forigin, :] = var_forecast_arr[:, 1, :].cpu()\n",
    "            actuals[:, forigin, :] = target_arr[:, 0, :].cpu()\n",
    "            actuals_train[:, forigin, :] = x_arr[:, 0, :].cpu()\n",
    "            #'''\n",
    "\n",
    "            mean_forecasts_scaled[:, forigin, :] = mean_forecast_arr_scaled[:, 0, :].cpu()\n",
    "            mean_forecasts_shifted_scaled[:, forigin, :] = mean_forecast_arr_scaled[:, 1, :].cpu()\n",
    "            var_forecasts_scaled[:, forigin, :] = var_forecast_arr_scaled[:, 0, :].cpu()\n",
    "            var_forecasts_shifted_scaled[:, forigin, :] = var_forecast_arr_scaled[:, 1, :].cpu()\n",
    "            actuals_scaled[:, forigin, :] = target_arr_scaled[:, 0, :].cpu()\n",
    "            actuals_shifted_scaled[:, forigin, :] = target_arr_scaled[:, 1, :].cpu()\n",
    "            actuals_train_scaled[:, forigin, :] = x_arr_scaled[:, 0, :].cpu()\n",
    "\n",
    "    \n",
    "        # Compute accuracy\n",
    "\n",
    "        smape = SMAPE(mean_forecasts_scaled, actuals_scaled)\n",
    "        smapc = SMAPE(mean_forecasts_scaled[: , :, :-1], mean_forecasts_shifted_scaled[: , :, 1:])\n",
    "        mape = MAPE(mean_forecasts_scaled, actuals_scaled)\n",
    "        mapc = MAPE(mean_forecasts_scaled[: , :, :-1], mean_forecasts_shifted_scaled[: , :, 1:])\n",
    "\n",
    "        rmsse = RMSSE(mean_forecasts_scaled, actuals_scaled, actuals_train_scaled)\n",
    "        rmssc = RMSSE(mean_forecasts_scaled[: , :, :-1], mean_forecasts_shifted_scaled[: , :, 1:], actuals_train_scaled)\n",
    "\n",
    "        crps = sCRPS(mean_forecasts_scaled, var_forecasts_scaled, actuals_scaled, actuals_train_scaled)\n",
    "\n",
    "        kl = (0.5 * GaussianKL_eval(mean_forecasts_scaled[: , :, :-1], var_forecasts_scaled[: , :, :-1],\n",
    "                                    mean_forecasts_shifted_scaled[: , :, 1:], var_forecasts_shifted_scaled[: , :, 1:]) +\n",
    "              0.5 * GaussianKL_eval(mean_forecasts_shifted_scaled[: , :, 1:], var_forecasts_shifted_scaled[: , :, 1:],\n",
    "                                    mean_forecasts_scaled[: , :, :-1], var_forecasts_scaled[: , :, :-1]))\n",
    "\n",
    "        print('sMAPE={:.4f} \\t sMAPC={:.4f}'.format(smape, smapc))\n",
    "        print('MAPE={:.4f} \\t MAPC={:.4f}'.format(mape, mapc))\n",
    "        print('RMSSE={:.4f} \\t RMSSC={:.4f}'.format(rmsse, rmssc))\n",
    "        print('CRPS={:.4f}'.format(crps))\n",
    "        print('KL={:.4f}'.format(kl))\n",
    "\n",
    "        wandb.log({\"sMAPE\": smape, \"sMAPC\": smapc,\n",
    "                   \"MAPE\": mape, \"MAPC\": mapc,\n",
    "                   \"RMSSE\": rmsse, \"RMSSC\": rmssc,\n",
    "                   \"CRPS\": crps,\n",
    "                   \"KL\": kl})\n",
    "\n",
    "        # n_series, forigin, forecast_length\n",
    "        fc_colnames = [str(i) for i in range(1, self.forecast_length + 1)]\n",
    "\n",
    "        actuals_np = actuals#.numpy()\n",
    "        actuals_np = actuals_scaled#.numpy()\n",
    "        m,n,r = actuals_np.shape\n",
    "        actuals_arr = np.column_stack((np.repeat(np.arange(m) + 1, n),\n",
    "                                       np.tile(np.arange(n) + 1, m),\n",
    "                                       actuals_np.reshape(m*n, -1)))\n",
    "        actuals_df = pd.DataFrame(actuals_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "        helper_col = ['actual'] * len(actuals_df)\n",
    "        actuals_df['type'] = helper_col\n",
    "\n",
    "        mean_forecasts_np = mean_forecasts#.numpy()\n",
    "        mean_forecasts_np = mean_forecasts_scaled#.numpy()\n",
    "        m,n,r = mean_forecasts_np.shape\n",
    "        mean_forecasts_arr = np.column_stack((np.repeat(np.arange(m) + 1, n),\n",
    "                                              np.tile(np.arange(n) + 1, m),\n",
    "                                              mean_forecasts_np.reshape(m*n, -1)))\n",
    "        mean_forecasts_df = pd.DataFrame(mean_forecasts_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "        helper_col = ['mean_forecast'] * len(mean_forecasts_df)\n",
    "        mean_forecasts_df['type'] = helper_col\n",
    "\n",
    "        sd_forecasts_np = np.sqrt(var_forecasts)#.numpy()\n",
    "        sd_forecasts_np = np.sqrt(var_forecasts_scaled)#.numpy()\n",
    "        m,n,r = sd_forecasts_np.shape\n",
    "        sd_forecasts_arr = np.column_stack((np.repeat(np.arange(m) + 1, n),\n",
    "                                            np.tile(np.arange(n) + 1, m),\n",
    "                                            sd_forecasts_np.reshape(m*n, -1)))\n",
    "        sd_forecasts_df = pd.DataFrame(sd_forecasts_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "        helper_col = ['sd_forecast'] * len(sd_forecasts_df)\n",
    "        sd_forecasts_df['type'] = helper_col\n",
    "\n",
    "        output_df_mm = pd.concat([actuals_df, mean_forecasts_df, sd_forecasts_df])\n",
    "\n",
    "        wandb.join()\n",
    "\n",
    "        return output_df_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAYZWWIG6FBn"
   },
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHaxwN7p6FBn"
   },
   "source": [
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIBfEjvN6FBn"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "wandb_project_name = 'nbeats_stability_prob'\n",
    "job_type_name = 'test_M4M' # one of 'validation_M3/4M' or 'test_M3/4M'\n",
    "\n",
    "hyperparameter_defaults = dict()\n",
    "hyperparameter_defaults['epochs'] = 155\n",
    "hyperparameter_defaults['batch_size'] = 512\n",
    "hyperparameter_defaults['nb_blocks_per_stack'] = 1\n",
    "hyperparameter_defaults['thetas_dims'] = 256\n",
    "hyperparameter_defaults['n_stacks'] = 20\n",
    "hyperparameter_defaults['share_weights_in_stack'] = False\n",
    "hyperparameter_defaults[\"backcast_length_multiplier\"] = 4\n",
    "hyperparameter_defaults['hidden_layer_units'] = 256\n",
    "hyperparameter_defaults['share_thetas'] = False\n",
    "hyperparameter_defaults[\"dropout\"] = False\n",
    "hyperparameter_defaults[\"dropout_p\"] = 0.0\n",
    "hyperparameter_defaults[\"neg_slope\"] = 0.00\n",
    "hyperparameter_defaults['learning_rate'] = 0.0005\n",
    "hyperparameter_defaults[\"weight_decay\"] = 0.00\n",
    "hyperparameter_defaults[\"LH\"] = 10\n",
    "hyperparameter_defaults[\"rndseed\"] = 0 #seeds used for paper: [1500,2500,3500,4500,5500,6500,7500,8500,9500,10500]\n",
    "hyperparameter_defaults[\"shifts\"] = 1\n",
    "hyperparameter_defaults['patience'] = 300 # Only affects validation runs\n",
    "hyperparameter_defaults['lambda'] = 0.15 # Rel. weight of forecast stability loss in loss_combined\n",
    "                                         # --> lambda_code = lambda_paper/(1+lambda_paper)\n",
    "                                         #e.g., if lambda_paper= 0.176 --> lambda_coda=0.15\n",
    "hyperparameter_defaults['training_stability'] = 'KL' #'WD' 'KL' 'RMSC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4RQ1koa6FBn"
   },
   "outputs": [],
   "source": [
    "if job_type_name == 'test_M3M':\n",
    "    is_val = False\n",
    "    mm_train, mm_eval = valset_m3m, testset_m3m\n",
    "elif job_type_name == 'validation_M3M':\n",
    "    is_val = True\n",
    "    mm_train, mm_eval = trainset_m3m, valset_m3m\n",
    "elif job_type_name == 'test_M4M':\n",
    "    is_val = False\n",
    "    mm_train, mm_eval = valset_m4m, testset_m4m\n",
    "elif job_type_name == 'validation_M4M':\n",
    "    is_val = True\n",
    "    mm_train, mm_eval = trainset_m4m, valset_m4m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PJa2fhV6FBn"
   },
   "source": [
    "Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YT8r0VM6FBn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sweep_function():\n",
    "    wandb.init(config = hyperparameter_defaults,\n",
    "              project = wandb_project_name,\n",
    "              job_type = job_type_name)\n",
    "    config = wandb.config\n",
    "    run_name = wandb.run.name\n",
    "    \n",
    "    # Initialize model\n",
    "    StableNBeatsGaussian_model = StableNBeatsGaussianLearner(device, 6, config)\n",
    "    # Train & evaluate\n",
    "    df_save = StableNBeatsGaussian_model.train_net(mm_train, mm_eval, 13, is_val)\n",
    "\n",
    "    df_save.to_csv('nbeats_stability_prob' + job_type_name + '_' + run_name + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-2Zt3I76FBp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"name\": \"m4_lambda\",\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"lambda\": {\n",
    "            \"values\": [0]\n",
    "        },\n",
    "      \"rndseed\": {\n",
    "            \"values\": [1500,2500,3500,4500,5500,6500,7500,8500,9500,10500]\n",
    "      },\n",
    "      \"training_stability\": {\n",
    "            \"values\": [\"RMSC\"] \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project = wandb_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqpLdfxV6FBp"
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function = sweep_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjeF3oFA6FBp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "124CaS0FkUzO2xT90ETyCFhss3hzfTdCT",
     "timestamp": 1617210264728
    },
    {
     "file_id": "1MurSROxqX19c-tUYi8AfIvz5787E8MwI",
     "timestamp": 1616840693200
    },
    {
     "file_id": "1XVbpATALPcZI62-iFvHT_j3rHBFKdvEs",
     "timestamp": 1616833964968
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
