{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyfkH9ThTbO0"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!pip install properscoring\n",
    "import properscoring as ps\n",
    "\n",
    "\n",
    "#!pip install -U \"gluonts[torch]\"\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_BHNUhDTbO6"
   },
   "source": [
    "# Data loading & data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_U-Gd0rqTbO7"
   },
   "outputs": [],
   "source": [
    "# Load monthly MM data.\n",
    "# Transform the data into a lists of arrays. Each inner array represents a timeseries.\n",
    "# Remove all the NaN values from the datasets.\n",
    "\n",
    "# M4\n",
    "trainset = pd.read_csv('https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Train/Monthly-train.csv')\n",
    "testset = pd.read_csv('https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Test/Monthly-test.csv')\n",
    "trainset.set_index('V1', inplace = True)\n",
    "testset.set_index('V1', inplace = True)\n",
    "# Add the testset columns behind the trainset columns\n",
    "testset_merge = trainset.merge(testset, on = 'V1', how = 'inner')\n",
    "# Get the data in numpy representation\n",
    "trainset_np = trainset.values\n",
    "testset_np = testset_merge.values\n",
    "# Select all non NaN values from the trainset\n",
    "trainset_clean = [x[x == x] for x in trainset_np]\n",
    "# Train/validation/test --------------------------------- NBeats paper validation strategy\n",
    "testset_m4m = [x[x == x] for x in testset_np]\n",
    "valset_m4m = trainset_clean.copy()\n",
    "trainset_m4m = [x[:-18] for x in trainset_clean]\n",
    "\n",
    "del(trainset, testset, testset_merge, trainset_np, testset_np, trainset_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryQr_ac-oIzP"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# For debugging & testing\n",
    "trainset_m4m = trainset_m4m[0:3]\n",
    "valset_m4m = valset_m4m[0:3]\n",
    "testset_m4m = testset_m4m[0:3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1DL2O_9lUPe"
   },
   "source": [
    "# DeepAR data & estimator config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJ3gt-cklX1_"
   },
   "outputs": [],
   "source": [
    "configDeepAR = dict()\n",
    "\n",
    "configDeepAR[\"type\"] = 'test_run' #'test_run' or 'validation_run'\n",
    "configDeepAR[\"run_name\"] = '32_context length'\n",
    "\n",
    "configDeepAR[\"rndseed\"] = 1500 #2500, 3500,4500,5500,6500,7500,8500,9500,10500\n",
    "\n",
    "configDeepAR[\"context_length\"] = 32\n",
    "configDeepAR[\"hidden_layers\"] = 3\n",
    "configDeepAR[\"hidden_layer_units\"] = 40 # 40 instead of 120 because of smaller decoder length!\n",
    "\n",
    "configDeepAR[\"learning_rate\"] = 1e-3\n",
    "configDeepAR[\"batch_size\"] = 32\n",
    "configDeepAR[\"epochs\"] = 1000\n",
    "configDeepAR[\"num_batches_per_epoch\"] = 100\n",
    "\n",
    "configDeepAR[\"sample_paths\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBblXC-WlcP2"
   },
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "from gluonts.torch.distributions.distribution_output import NormalOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMxb_km9lrbP"
   },
   "outputs": [],
   "source": [
    "start = \"01-01-2023\" # add \"target\" and \"start\" fields --> doesn't work anymore with 0\n",
    "frequency = '1M'\n",
    "\n",
    "if configDeepAR[\"type\"]=='validation_run':\n",
    "    training_data = trainset_m4m\n",
    "    test_data = valset_m4m\n",
    "elif configDeepAR[\"type\"]=='test_run':\n",
    "    training_data = valset_m4m\n",
    "    test_data = testset_m4m\n",
    "\n",
    "# train dataset\n",
    "train_ds = ListDataset(\n",
    "    [{'target': x,'start':start} for x in training_data],\n",
    "    freq=frequency\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# test datasets\n",
    "test_ds = dict()\n",
    "for fold in range(1,13):\n",
    "    ds_name = 'test' + str(fold) + '_ds'\n",
    "    test_data_fold = [x[:-(13-fold)] for x in test_data]\n",
    "    test_ds[ds_name] = ListDataset(\n",
    "        [{'target': x, 'start': start} for x in test_data_fold],\n",
    "        freq=frequency\n",
    "    )\n",
    "test_ds['test13_ds'] = ListDataset(\n",
    "    [{'target': x, 'start': start} for x in test_data],\n",
    "    freq=frequency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJc9TMA3lxXL"
   },
   "outputs": [],
   "source": [
    "from gluonts.torch.modules.loss import DistributionLoss\n",
    "estimator = DeepAREstimator(\n",
    "    freq = frequency,\n",
    "    prediction_length = 6,\n",
    "    context_length = configDeepAR[\"context_length\"],\n",
    "    num_layers = configDeepAR[\"hidden_layers\"],\n",
    "    hidden_size = configDeepAR[\"hidden_layer_units\"],\n",
    "    dropout_rate = 0.0,\n",
    "    distr_output = NormalOutput(),\n",
    "    scaling = True,\n",
    "    batch_size = configDeepAR[\"batch_size\"],\n",
    "    lr = configDeepAR[\"learning_rate\"],\n",
    "    patience = 20,\n",
    "    num_batches_per_epoch = configDeepAR[\"num_batches_per_epoch\"],\n",
    "trainer_kwargs={\"max_epochs\": configDeepAR[\"epochs\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpFaj4KdTbO-"
   },
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6j9XI8BRmD2E"
   },
   "outputs": [],
   "source": [
    "seed_value = configDeepAR[\"rndseed\"]\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d02gspbhTbO_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = estimator.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxAVGnp4TbPA"
   },
   "outputs": [],
   "source": [
    "from gluonts.evaluation import make_evaluation_predictions\n",
    "from gluonts.model.forecast import SampleForecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl-IG2PnmMfY"
   },
   "outputs": [],
   "source": [
    "ts_dict = dict()\n",
    "forecast_dict = dict()\n",
    "\n",
    "for fold in range(1,14):\n",
    "    print(\"fold: \" + str(fold))\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset = test_ds['test' + str(fold) + '_ds'],\n",
    "        predictor = predictor,\n",
    "        num_samples = configDeepAR[\"sample_paths\"],\n",
    "    )\n",
    "\n",
    "    ts_list = list()\n",
    "    forecast_list = list()\n",
    "\n",
    "    for i in range(len(test_ds['test' + str(fold) + '_ds'])):\n",
    "        ts_list.append(next(ts_it).values.squeeze())\n",
    "        forecast_list.append(next(forecast_it).samples)\n",
    "\n",
    "    ts_dict['ts_list_' + str(fold)] = ts_list\n",
    "    forecast_dict['forecast_list_' + str(fold)] = forecast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHi03ZuQMfAv"
   },
   "outputs": [],
   "source": [
    "# Containers to save actuals and forecasts\n",
    "# n_series, forigin, forecast_length\n",
    "actuals_np = np.empty(shape = (len(test_data), 13, 6))\n",
    "mean_forecasts_np = np.empty(shape = (len(test_data), 13, 6))\n",
    "sd_forecasts_np = np.empty(shape = (len(test_data), 13, 6))\n",
    "csd_forecasts_np = np.empty(shape = (len(test_data), 13, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcmUTrWXob14"
   },
   "outputs": [],
   "source": [
    "for origin in range(1,14):\n",
    "    helper_actuals = [x[-6:] for x in ts_dict['ts_list_' + str(origin)]]\n",
    "    actuals_np[:, (origin-1), :] = np.array(helper_actuals)\n",
    "\n",
    "    forecasts_samples_origin = np.array(forecast_dict['forecast_list_' + str(origin)])\n",
    "    mean_forecasts_np[:, (origin-1), :] = np.mean(forecasts_samples_origin, axis = 1)\n",
    "    sd_forecasts_np[:, (origin-1), :] = np.std(forecasts_samples_origin, axis = 1)\n",
    "    csd_forecasts_np[:, (origin-1), :] = np.std(np.cumsum(forecasts_samples_origin, axis = 2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAVNmic-MycE"
   },
   "outputs": [],
   "source": [
    "# n_series, forigin, forecast_length\n",
    "fc_colnames = [str(i) for i in range(1, 7)]\n",
    "\n",
    "m,n,r = actuals_np.shape\n",
    "actuals_arr = np.column_stack((np.repeat(np.arange(m) + 1, n),\n",
    "                               np.tile(np.arange(n) + 1, m),\n",
    "                               actuals_np.reshape(m*n, -1)))\n",
    "actuals_df = pd.DataFrame(actuals_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "helper_col = ['actual'] * len(actuals_df)\n",
    "actuals_df['type'] = helper_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECSoSZC6Myes"
   },
   "outputs": [],
   "source": [
    "# n_series, forigin, forecast_length\n",
    "fc_colnames = [str(i) for i in range(1, 7)]\n",
    "\n",
    "m,n,r = mean_forecasts_np.shape\n",
    "mean_forecasts_arr = np.column_stack((np.repeat(np.arange(m) + 1, n),\n",
    "                                      np.tile(np.arange(n) + 1, m),\n",
    "                                      mean_forecasts_np.reshape(m*n, -1)))\n",
    "mean_forecasts_df = pd.DataFrame(mean_forecasts_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "helper_col = ['mean_forecast'] * len(mean_forecasts_df)\n",
    "mean_forecasts_df['type'] = helper_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bgRf2LuMyhk"
   },
   "outputs": [],
   "source": [
    "# n_series, forigin, forecast_length\n",
    "fc_colnames = [str(i) for i in range(1, 7)]\n",
    "\n",
    "m,n,r = sd_forecasts_np.shape\n",
    "sd_forecasts_arr = np.column_stack((np.repeat(np.arange(m) + 1, n),\n",
    "                                    np.tile(np.arange(n) + 1, m),\n",
    "                                    sd_forecasts_np.reshape(m*n, -1)))\n",
    "sd_forecasts_df = pd.DataFrame(sd_forecasts_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "helper_col = ['sd_forecast'] * len(sd_forecasts_df)\n",
    "sd_forecasts_df['type'] = helper_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BW7Ut4NfM3dS"
   },
   "outputs": [],
   "source": [
    "# n_series, forigin, forecast_length\n",
    "fc_colnames = [str(i) for i in range(1, 7)]\n",
    "\n",
    "m,n,r = csd_forecasts_np.shape\n",
    "csd_forecasts_arr = np.column_stack((np.repeat(np.arange(m) + 1, n),\n",
    "                                     np.tile(np.arange(n) + 1, m),\n",
    "                                     csd_forecasts_np.reshape(m*n, -1)))\n",
    "csd_forecasts_df = pd.DataFrame(csd_forecasts_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "helper_col = ['csd_forecast'] * len(csd_forecasts_df)\n",
    "csd_forecasts_df['type'] = helper_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrQXG4LKM3fp"
   },
   "outputs": [],
   "source": [
    "output_df_mm = pd.concat([actuals_df, mean_forecasts_df, sd_forecasts_df, csd_forecasts_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ek1rquAmvYU"
   },
   "outputs": [],
   "source": [
    "output_df_mm.to_csv('deepAR_prob_' + configDeepAR[\"type\"] + '_' + configDeepAR[\"run_name\"] + \"gluonts_pytorch_1000j\"+ '.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
